{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DAY-3 Deep Convolution GAN .ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPW2rbqCsi6rdwcDsyZrgW5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ajay-Sai-Kiran/GANS/blob/main/DAY_3_Deep_Convolution_GAN_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Types of GANS"
      ],
      "metadata": {
        "id": "YJEZcMizZLKb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Foundation\n",
        "\n",
        "Generative Adversarial Network (GAN)\n",
        "\n",
        "Deep Convolutional Generative Adversarial Network (DCGAN)\n",
        "\n",
        "#Extensions\n",
        "Conditional Generative Adversarial Network (cGAN)\n",
        "\n",
        "Information Maximizing Generative Adversarial Network (InfoGAN)\n",
        "\n",
        "Auxiliary Classifier Generative Adversarial Network (AC-GAN)\n",
        "Stacked Generative Adversarial Network (StackGAN)\n",
        "\n",
        "\n",
        "Context Encoders\n",
        "\n",
        "Pix2Pix\n",
        "\n",
        "#Advanced\n",
        "\n",
        "Wasserstein Generative Adversarial Network (WGAN)\n",
        "\n",
        "Cycle-Consistent Generative Adversarial Network (CycleGAN)\n",
        "\n",
        "Progressive Growing Generative Adversarial Network (Progressive GAN)\n",
        "\n",
        "Style-Based Generative Adversarial Network (StyleGAN)\n",
        "\n",
        "Big Generative Adversarial Network (BigGAN)\n"
      ],
      "metadata": {
        "id": "2IyTvZD0ZNbM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Deep Convolutional Generative Adversarial Network (DCGAN)"
      ],
      "metadata": {
        "id": "nk8P59ceZXr7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The deep convolutional generative adversarial network, or DCGAN for short, is an extension of the GAN architecture for using deep convolutional neural networks for both the generator and discriminator models and configurations for the models and training that result in the stable training of a generator model"
      ],
      "metadata": {
        "id": "fefkI2y1Za5b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DCGAN uses **convolutional and convolutional-transpose layers** in the generator and discriminator, respectively."
      ],
      "metadata": {
        "id": "e7dFJ-KSZloL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here the **discriminator consists of strided convolution layers, batch normalization layers, and LeakyRelu as activation function**. It takes a 3x64x64 input image. \n",
        "\n",
        "The role of the discriminator here is to determine that the image comes from either real dataset or generator.   The discriminator can be simply designed similar to a convolution neural network that performs a image classification task.\n",
        "\n"
      ],
      "metadata": {
        "id": "THIhdCi6aGmi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **generator consists of convolutional-transpose layers, batch normalization layers, and ReLU activations**. The output will be a 3x64x64 RGB image."
      ],
      "metadata": {
        "id": "rAhgoFZ1ixTh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The generator of the DCGAN architecture takes 100 uniform generated values using normal distribution as an input. \n",
        "\n",
        "First, it changes the dimension  to 4x4x1024 and performed a fractionally strided convolution in 4 times with stride of 1/2 (this means every time when applied, it doubles the image dimension while reducing the number of output channels). \n",
        "\n",
        "The generated output has dimensions of (64, 64, 3). There are some architectural changes proposed in generator such as removal of all fully connected layer, use of Batch Normalization which helps in stabilizing training."
      ],
      "metadata": {
        "id": "Hz1Ji-KhikHl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Implementation"
      ],
      "metadata": {
        "id": "m11j55CKipN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code % matplotlib inline\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from IPython import display\n",
        "\n",
        "# Check tensorflow version\n",
        "print('Tensorflow version:', tf.__version__)\n"
      ],
      "metadata": {
        "id": "HAMWZno-i0mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we load the fashion-MNIST dataset, the good thing is that dataset can be imported  from tf.keras.datasets API. So, we donâ€™t need to load datasets manually by copying files. This dateset contains 60k training images and 10k test images each of dimensions(28, 28, 1). Since the value of each pixel is in the range (0, 255), we divide these values by 255 to normalize it."
      ],
      "metadata": {
        "id": "pGcw9dJ5x4Av"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "x_train = x_train.astype(np.float32) / 255.0\n",
        "x_test = x_test.astype(np.float32) / 255.0\n",
        "x_train.shape, x_test.shape\n"
      ],
      "metadata": {
        "id": "JjB3oU24rKHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we will  be visualizing some of the images from Fashion-MNIST dateset, we use matplotlib library for that."
      ],
      "metadata": {
        "id": "XRDk5c00yDJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We plot first 25 images of training dataset\n",
        "plt.figure(figsize =(10, 10))\n",
        "for i in range(25):\n",
        "\tplt.subplot(5, 5, i + 1)\n",
        "\tplt.xticks([])\n",
        "\tplt.yticks([])\n",
        "\tplt.grid(False)\n",
        "\tplt.imshow(x_train[i], cmap = plt.cm.binary)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GPtf2n9frQQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we define training parameters such as batch size and divides the dataset into batch size and fills those batch size by randomly sampling the training data."
      ],
      "metadata": {
        "id": "em_nBPFZyHFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code\n",
        "batch_size = 32\n",
        "# This dataset fills a buffer with buffer_size elements,\n",
        "# then randomly samples elements from this buffer,\n",
        "# replacing the selected elements with new elements.\n",
        "def create_batch(x_train):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(x_train).shuffle(1000)\n",
        "# Combines consecutive elements of this dataset into batches.\n",
        "\n",
        "  dataset = dataset.batch(batch_size, drop_remainder = True).prefetch(1)\n",
        "# Creates a Dataset that prefetches elements from this dataset\n",
        "  return dataset\n"
      ],
      "metadata": {
        "id": "C6T9l6s6rWS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we define the generator architecture, this generator architecture takes a vector of size 100 and first reshape that into (7, 7, 128) vector then applied transpose convolution in combination with batch normalization. The output of this generator is a trained an image of dimension (28, 28, 1)."
      ],
      "metadata": {
        "id": "Q1kz_yPayJUn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The strided conv-transpose layers allow the latent vector to be transformed into a volume with the same shape as an image."
      ],
      "metadata": {
        "id": "yxpgBIgA1G2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_features = 100\n",
        "\n",
        "generator = keras.models.Sequential([\n",
        "\tkeras.layers.Dense(7 * 7 * 128, input_shape =[num_features]),\n",
        "\tkeras.layers.Reshape([7, 7, 128]),\n",
        "\tkeras.layers.BatchNormalization(),\n",
        "\tkeras.layers.Conv2DTranspose(\n",
        "\t\t64, (5, 5), (2, 2), padding =\"same\", activation =\"selu\"),\n",
        "\tkeras.layers.BatchNormalization(),\n",
        "\tkeras.layers.Conv2DTranspose(\n",
        "\t\t1, (5, 5), (2, 2), padding =\"same\", activation =\"tanh\"),\n",
        "])\n",
        "generator.summary()\n"
      ],
      "metadata": {
        "id": "S32nzyNNrgV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we define out discriminator architecture, the discriminator takes image of size  28*28 with 1 color channel and output a scalar value representing image from either dataset or generated image."
      ],
      "metadata": {
        "id": "mrMwLHfpySSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = keras.models.Sequential([\n",
        "\tkeras.layers.Conv2D(64, (5, 5), (2, 2), padding =\"same\", input_shape =[28, 28, 1]),\n",
        "\tkeras.layers.LeakyReLU(0.2),\n",
        "\tkeras.layers.Dropout(0.3),\n",
        "\tkeras.layers.Conv2D(128, (5, 5), (2, 2), padding =\"same\"),\n",
        "\tkeras.layers.LeakyReLU(0.2),\n",
        "\tkeras.layers.Dropout(0.3),\n",
        "\tkeras.layers.Flatten(),\n",
        "\tkeras.layers.Dense(1, activation ='sigmoid')\n",
        "])\n",
        "discriminator.summary()\n"
      ],
      "metadata": {
        "id": "cEllv16mrpJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we need to compile the our DCGAN model (combination of generator and discriminator), we will first compile discriminator and set its  training to False, because we first want to train the generator."
      ],
      "metadata": {
        "id": "E8HMFMCWyXee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compile discriminator using binary cross entropy loss and adam optimizer\n",
        "discriminator.compile(loss =\"binary_crossentropy\", optimizer =\"adam\")\n",
        "# make discriminator no-trainable as of now\n",
        "discriminator.trainable = False\n",
        "# Combine both generator and discriminator\n",
        "gan = keras.models.Sequential([generator, discriminator])\n",
        "# compile generator using binary cross entropy loss and adam optimizer\n",
        "\n",
        "gan.compile(loss =\"binary_crossentropy\", optimizer =\"adam\")\n"
      ],
      "metadata": {
        "id": "rlnxMhYKruT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we define the training procedure for this GAN model, we will be using tqdm package which we have imported earlier., this package help in visualizing training."
      ],
      "metadata": {
        "id": "yNixBBtLzKGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = tf.random.normal(shape =[batch_size, 100])\n",
        "\n",
        "def train_dcgan(gan, dataset, batch_size, num_features, epochs = 5):\n",
        "\tgenerator, discriminator = gan.layers\n",
        "\tfor epoch in tqdm(range(epochs)):\n",
        "\t\tprint()\n",
        "\t\tprint(\"Epoch {}/{}\".format(epoch + 1, epochs))\n",
        "\n",
        "\t\tfor X_batch in dataset:\n",
        "\t\t\t# create a random noise of sizebatch_size * 100\n",
        "\t\t\t# to passit into the generator\n",
        "\t\t\tnoise = tf.random.normal(shape =[batch_size, num_features])\n",
        "\t\t\tgenerated_images = generator(noise)\n",
        "\n",
        "\t\t\t# take batch of generated image and real image and\n",
        "\t\t\t# use them to train the discriminator\n",
        "\t\t\tX_fake_and_real = tf.concat([generated_images, X_batch], axis = 0)\n",
        "\t\t\ty1 = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)\n",
        "\t\t\tdiscriminator.trainable = True\n",
        "\t\t\tdiscriminator.train_on_batch(X_fake_and_real, y1)\n",
        "\n",
        "\t\t\t# Here we will be training our GAN model, in this step\n",
        "\t\t\t# we pass noise that uses geeneratortogenerate the image\n",
        "\t\t\t# and pass it with labels as [1] So, it can fool the discriminatoe\n",
        "\t\t\tnoise = tf.random.normal(shape =[batch_size, num_features])\n",
        "\t\t\ty2 = tf.constant([[1.]] * batch_size)\n",
        "\t\t\tdiscriminator.trainable = False\n",
        "\t\t\tgan.train_on_batch(noise, y2)\n",
        "\n",
        "\t\t\t# generate images for the GIF as we go\n",
        "\t\t\tgenerate_and_save_images(generator, epoch + 1, seed)\n",
        "\n",
        "\tgenerate_and_save_images(generator, epochs, seed)\n"
      ],
      "metadata": {
        "id": "0kpsPlcCsmkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we define a function that generate and save images from generator (during training). We will use these generated images to plot the GIF later."
      ],
      "metadata": {
        "id": "Z8acUcGszpU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code\n",
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  predictions = model(test_input, training = False)\n",
        "\n",
        "  fig = plt.figure(figsize =(10, 10))\n",
        "\n",
        "  for i in range(25):\n",
        "\t  plt.subplot(5, 5, i + 1)\n",
        "\t  plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap ='binary')\n",
        "\t  plt.axis('off')\n",
        "\n",
        "  plt.savefig('image_epoch_{:04d}.png'.format(epoch))\n"
      ],
      "metadata": {
        "id": "LC0LPV0vstAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we need to train the model but before that we also need to create batches of training data and add a dimension that represents number  of color maps."
      ],
      "metadata": {
        "id": "blwxzFw4zqpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reshape to add a color map\n",
        "x_train_dcgan = x_train.reshape(-1, 28, 28, 1) * 2. - 1.\n",
        "# create batches\n",
        "dataset = create_batch(x_train_dcgan)\n",
        "# callthe training function with 10 epochs and record time %% time\n",
        "train_dcgan(gan, dataset, batch_size, num_features, epochs = 10)\n"
      ],
      "metadata": {
        "id": "g_LNvRlgs27Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "import glob\n",
        "\n",
        "anim_file = 'dcgan_results.gif'\n",
        "\n",
        "with imageio.get_writer(anim_file, mode ='I') as writer:\n",
        "filenames = glob.glob('image*.png')\n",
        "filenames = sorted(filenames)\n",
        "last = -1\n",
        "for i, filename in enumerate(filenames):\n",
        "\tframe = 2*(i)\n",
        "\tif round(frame) > round(last):\n",
        "\tlast = frame\n",
        "\telse:\n",
        "\tcontinue\n",
        "\timage = imageio.imread(filename)\n",
        "\twriter.append_data(image)\n",
        "image = imageio.imread(filename)\n",
        "writer.append_data(image)\n",
        "display.Image(filename = anim_file)\n"
      ],
      "metadata": {
        "id": "8nSd8NvEvqB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reference:\n",
        "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks\n",
        "\n",
        "by Alec Radford, Luke Metz, Soumith Chintala"
      ],
      "metadata": {
        "id": "kehbYligvwtm"
      }
    }
  ]
}